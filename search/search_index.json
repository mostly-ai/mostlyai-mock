{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Synthetic Mock Data \ud83d\udd2e","text":"<p>Use LLMs to generate any Tabular Data towards your needs. Create from scratch, expand existing datasets, or enrich tables with new columns. Your prompts, your rules, your data.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>A light-weight python client for prompting LLMs for mixed-type tabular data.</li> <li>Select from a wide range of LLM endpoints and LLM models.</li> <li>Supports single-table as well as multi-table scenarios.</li> <li>Supports variety of data types: <code>string</code>, <code>integer</code>, <code>float</code>, <code>category</code>, <code>boolean</code>, <code>date</code>, and <code>datetime</code>.</li> <li>Specify context, distributions and rules via dataset-, table- or column-level prompts.</li> <li>Create from scratch or enrich existing datasets with new columns and/or rows.</li> <li>Tailor the diversity and realism of your generated data via temperature and top_p.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install the latest version of the <code>mostlyai-mock</code> python package.</li> </ol> <pre><code>pip install -U mostlyai-mock\n</code></pre> <ol> <li>Set the API key of your LLM endpoint (if not done yet)</li> </ol> <pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n# os.environ[\"GEMINI_API_KEY\"] = \"your-api-key\"\n# os.environ[\"GROQ_API_KEY\"] = \"your-api-key\"\n</code></pre> <p>Note: You will need to obtain your API key directly from the LLM service provider (e.g. for Open AI from here). The LLM endpoint will be determined by the chosen <code>model</code> when making calls to <code>mock.sample</code>.</p> <ol> <li>Create your first basic mock table from scratch</li> </ol> <pre><code>from mostlyai import mock\n\ntables = {\n    \"guests\": {\n        \"prompt\": \"Guests of an Alpine ski hotel in Austria\",\n        \"columns\": {\n            \"nationality\": {\"prompt\": \"2-letter code for the nationality\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"first name and last name of the guest\", \"dtype\": \"string\"},\n            \"gender\": {\"dtype\": \"category\", \"values\": [\"male\", \"female\"]},\n            \"age\": {\"prompt\": \"age in years; min: 18, max: 80; avg: 25\", \"dtype\": \"integer\"},\n            \"date_of_birth\": {\"prompt\": \"date of birth\", \"dtype\": \"date\"},\n            \"checkin_time\": {\"prompt\": \"the check in timestamp of the guest; may 2025\", \"dtype\": \"datetime\"},\n            \"is_vip\": {\"prompt\": \"is the guest a VIP\", \"dtype\": \"boolean\"},\n            \"price_per_night\": {\"prompt\": \"price paid per night, in EUR\", \"dtype\": \"float\"},\n            \"room_number\": {\"prompt\": \"room number\", \"dtype\": \"integer\", \"values\": [101, 102, 103, 201, 202, 203, 204]}\n        },\n    }\n}\ndf = mock.sample(\n    tables=tables,   # provide table and column definitions\n    sample_size=10,  # generate 10 records\n    model=\"openai/gpt-5-nano\",  # select the LLM model (optional)\n)\nprint(df)\n#   nationality                 name  gender  age date_of_birth        checkin_time is_vip  price_per_night  room_number\n# 0          FR          Jean Dupont    male   29    1994-03-15 2025-01-10 14:30:00  False            150.0          101\n# 1          DE         Anna Schmidt  female   34    1989-07-22 2025-01-11 16:45:00   True            200.0          201\n# 2          IT          Marco Rossi    male   45    1979-11-05 2025-01-09 10:15:00  False            180.0          102\n# 3          AT         Laura Gruber  female   28    1996-02-19 2025-01-12 09:00:00  False            165.0          202\n# 4          CH         David M\u00fcller    male   37    1987-08-30 2025-01-08 17:20:00   True            210.0          203\n# 5          NL  Sophie van den Berg  female   22    2002-04-12 2025-01-10 12:00:00  False            140.0          103\n# 6          GB         James Carter    male   31    1992-09-10 2025-01-11 11:30:00  False            155.0          204\n# 7          BE        Lotte Peeters  female   26    1998-05-25 2025-01-09 15:45:00  False            160.0          201\n# 8          DK        Anders Jensen    male   33    1990-12-03 2025-01-12 08:15:00   True            220.0          202\n# 9          ES         Carlos Lopez    male   38    1985-06-14 2025-01-10 18:00:00  False            170.0          203\n</code></pre> <ol> <li>Create your first multi-table mock dataset</li> </ol> <pre><code>from mostlyai import mock\n\ntables = {\n    \"customers\": {\n        \"prompt\": \"Customers of a hardware store\",\n        \"columns\": {\n            \"customer_id\": {\"prompt\": \"the unique id of the customer\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"first name and last name of the customer\", \"dtype\": \"string\"},\n        },\n        \"primary_key\": \"customer_id\",\n    },\n    \"warehouses\": {\n        \"prompt\": \"Warehouses of a hardware store\",\n        \"columns\": {\n            \"warehouse_id\": {\"prompt\": \"the unique id of the warehouse\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"the name of the warehouse\", \"dtype\": \"string\"},\n        },\n        \"primary_key\": \"warehouse_id\",\n    },\n    \"orders\": {\n        \"prompt\": \"Orders of a Customer\",\n        \"columns\": {\n            \"customer_id\": {\"prompt\": \"the customer id for that order\", \"dtype\": \"string\"},\n            \"warehouse_id\": {\"prompt\": \"the warehouse id for that order\", \"dtype\": \"string\"},\n            \"order_id\": {\"prompt\": \"the unique id of the order\", \"dtype\": \"string\"},\n            \"text\": {\"prompt\": \"order text description\", \"dtype\": \"string\"},\n            \"amount\": {\"prompt\": \"order amount in USD\", \"dtype\": \"float\"},\n        },\n        \"primary_key\": \"order_id\",\n        \"foreign_keys\": [\n            {\n                \"column\": \"customer_id\",\n                \"referenced_table\": \"customers\",\n                \"prompt\": \"each customer has anywhere between 2 and 3 orders\",\n            },\n            {\n                \"column\": \"warehouse_id\",\n                \"referenced_table\": \"warehouses\",\n            },\n        ],\n    },\n    \"items\": {\n        \"prompt\": \"Items in an Order\",\n        \"columns\": {\n            \"item_id\": {\"prompt\": \"the unique id of the item\", \"dtype\": \"string\"},\n            \"order_id\": {\"prompt\": \"the order id for that item\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"the name of the item\", \"dtype\": \"string\"},\n            \"price\": {\"prompt\": \"the price of the item in USD\", \"dtype\": \"float\"},\n        },\n        \"foreign_keys\": [\n            {\n                \"column\": \"order_id\",\n                \"referenced_table\": \"orders\",\n                \"prompt\": \"each order has between 1 and 2 items\",\n            }\n        ],\n        \"primary_key\": \"item_id\",\n    },\n}\ndata = mock.sample(\n    tables=tables,\n    sample_size=2,\n    model=\"openai/gpt-5\",\n    n_workers=1,\n)\nprint(data[\"customers\"])\n#   customer_id             name\n# 0   B0-100235  Danielle Rogers\n# 1   B0-100236       Edward Kim\nprint(data[\"warehouses\"])\n#   warehouse_id                          name\n# 0       B0-001  Downtown Distribution Center\n# 1       B0-002     Westside Storage Facility\nprint(data[\"orders\"])\n#   customer_id warehouse_id    order_id                                               text   amount\n# 0   B0-100235       B0-002  B0-3010021  Office furniture replenishment - desks, chairs...  1268.35\n# 1   B0-100235       B0-001  B0-3010022  Bulk stationery order: printer paper, notebook...    449.9\n# 2   B0-100235       B0-001  B0-3010023  Electronics restock: monitors and wireless key...    877.6\n# 3   B0-100236       B0-001  B1-3010021  Monthly cleaning supplies: disinfectant, trash...   314.75\n# 4   B0-100236       B0-002  B1-3010022  Breakroom essentials restock: coffee, tea, and...   182.45\nprint(data[\"items\"])\n#      item_id    order_id                                   name   price\n# 0  B0-200501  B0-3010021                  Ergonomic Office Desk  545.99\n# 1  B0-200502  B0-3010021              Mesh Back Executive Chair   399.5\n# 2  B1-200503  B0-3010022   Multipack Printer Paper (500 sheets)  129.95\n# 3  B1-200504  B0-3010022             Spiral Notebooks - 12 Pack   59.99\n# 4  B2-200505  B0-3010023               27\" LED Computer Monitor  489.95\n# 5  B2-200506  B0-3010023            Wireless Ergonomic Keyboard  387.65\n# 6  B3-200507  B1-3010021  Industrial Disinfectant Solution (5L)  148.95\n# 7  B3-200508  B1-3010021  Commercial Trash Liners - Case of 100    84.5\n# 8  B4-200509  B1-3010022        Premium Ground Coffee (2lb Bag)   74.99\n# 9  B4-200510  B1-3010022         Bottled Spring Water (24 Pack)   34.95\n</code></pre> <ol> <li>Create your first self-referencing mock table with auto-increment integer primary keys</li> </ol> <pre><code>from mostlyai import mock\n\ntables = {\n    \"employees\": {\n        \"prompt\": \"Employees of a company\",\n        \"columns\": {\n            \"employee_id\": {\"dtype\": \"integer\"},\n            \"name\": {\"prompt\": \"first name and last name of the employee\", \"dtype\": \"string\"},\n            \"boss_id\": {\"dtype\": \"integer\"},\n            \"role\": {\"prompt\": \"the role of the employee\", \"dtype\": \"string\"},\n        },\n        \"primary_key\": \"employee_id\",\n        \"foreign_keys\": [\n            {\n                \"column\": \"boss_id\",\n                \"referenced_table\": \"employees\",\n                \"prompt\": \"each boss has at most 3 employees\",\n            },\n        ],\n    }\n}\ndf = mock.sample(tables=tables, sample_size=10, model=\"openai/gpt-5\", n_workers=1)\nprint(df)\n#   employee_id              name  boss_id                   role\n# 0            1      Patricia Lee     &lt;NA&gt;              President\n# 1            2  Edward Rodriguez        1       VP of Operations\n# 2            3      Maria Cortez        1          VP of Finance\n# 3            4     Thomas Nguyen        1       VP of Technology\n# 4            5        Rachel Kim        2     Operations Manager\n# 5            6     Jeffrey Patel        2      Supply Chain Lead\n# 6            7      Olivia Smith        2  Facilities Supervisor\n# 7            8      Brian Carter        3     Accounting Manager\n# 8            9   Lauren Anderson        3      Financial Analyst\n# 9           10   Santiago Romero        3     Payroll Specialist\n</code></pre> <ol> <li>Enrich existing data with additional columns</li> </ol> <pre><code>from mostlyai import mock\nimport pandas as pd\n\ntables = {\n    \"guests\": {\n        \"prompt\": \"Guests of an Alpine ski hotel in Austria\",\n        \"columns\": {\n            \"gender\": {\"dtype\": \"category\", \"values\": [\"male\", \"female\"]},\n            \"age\": {\"prompt\": \"age in years; min: 18, max: 80; avg: 25\", \"dtype\": \"integer\"},\n            \"room_number\": {\"prompt\": \"room number\", \"dtype\": \"integer\"},\n            \"is_vip\": {\"prompt\": \"is the guest a VIP\", \"dtype\": \"boolean\"},\n        },\n        \"primary_key\": \"guest_id\",\n    }\n}\nexisting_guests = pd.DataFrame({\n    \"guest_id\": [1, 2, 3],\n    \"name\": [\"Anna Schmidt\", \"Marco Rossi\", \"Sophie Dupont\"],\n    \"nationality\": [\"DE\", \"IT\", \"FR\"],\n})\ndf = mock.sample(\n    tables=tables,\n    existing_data={\"guests\": existing_guests},\n    model=\"openai/gpt-5-nano\"\n)\nprint(df)\n#   guest_id           name nationality  gender  age  room_number is_vip\n# 0        1   Anna Schmidt          DE  female   30          102  False\n# 1        2    Marco Rossi          IT    male   27          215   True\n# 2        3  Sophie Dupont          FR  female   22          108  False\n</code></pre>"},{"location":"#mcp-server","title":"MCP Server","text":"<p>This repo comes with MCP Server. It can be easily consumed by any MCP Client by providing the following configuration:</p> <pre><code>{\n    \"mcpServers\": {\n        \"mostlyai-mock-mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [\"--from\", \"mostlyai-mock[mcp]\", \"mcp-server\"],\n            \"env\": {\n                \"OPENAI_API_KEY\": \"PROVIDE YOUR KEY\",\n                \"GEMINI_API_KEY\": \"PROVIDE YOUR KEY\",\n                \"GROQ_API_KEY\": \"PROVIDE YOUR KEY\",\n                \"ANTHROPIC_API_KEY\": \"PROVIDE YOUR KEY\"\n            }\n        }\n    }\n}\n</code></pre> <p>For example: - in Claude Desktop, go to \"Settings\" &gt; \"Developer\" &gt; \"Edit Config\" and paste the above into <code>claude_desktop_config.json</code> - in Cursor, go to \"Settings\" &gt; \"Cursor Settings\" &gt; \"MCP\" &gt; \"Add new global MCP server\" and paste the above into <code>mcp.json</code></p> <p>Troubleshooting: 1. If the MCP Client fails to detect the MCP Server, provide the absolute path in the <code>command</code> field, for example: <code>/Users/johnsmith/.local/bin/uvx</code> 2. To debug MCP Server issues, you can use MCP Inspector by running: <code>npx @modelcontextprotocol/inspector -- uvx --from mostlyai-mock[mcp] mcp-server</code> 3. In order to develop locally, modify the configuration by replacing <code>\"command\": \"uv\"</code> (or use the full path to <code>uv</code> if needed) and <code>\"args\": [\"--directory\", \"/Users/johnsmith/mostlyai-mock\", \"run\", \"--extra\", \"mcp\", \"mcp-server\"]</code></p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#engine-reference","title":"Engine Reference","text":""},{"location":"api/#mostlyai.mock.sample","title":"mostlyai.mock.sample","text":"<pre><code>sample(\n    *,\n    tables,\n    sample_size=4,\n    existing_data=None,\n    model=\"openai/gpt-5-nano\",\n    api_key=None,\n    temperature=1.0,\n    top_p=0.95,\n    n_workers=10,\n    return_type=\"auto\",\n    progress_callback=None\n)\n</code></pre> <p>Generate synthetic data from scratch or enrich existing data with new columns.</p> <p>While faker and numpy are useful to create fake data, this utility is unique as it allows the creation of coherent, realistic multi-table tabular mock data or the enrichment of existing datasets with new, context-aware columns.</p> <p>It is particularly useful for quickly simulating production-like datasets for testing or prototyping purposes. It is advised to limit mocking to small datasets for performance reasons (rows * cols &lt; 1000). It might take a couple of minutes for bigger datasets.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>dict[str, dict]</code> <p>The table specifications to generate mock data for. See examples for usage. Note: Avoid using double quotes (<code>\"</code>) and other special characters in column names. Available dtypes: <code>string</code>, <code>integer</code>, <code>float</code>, <code>category</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>. Primary key dtypes: <code>integer</code> \u2192 auto-increment (1, 2, 3, ...); <code>string</code> \u2192 LLM-generated unique IDs.</p> required <code>sample_size</code> <code>int | dict[str, int]</code> <p>The number of rows to generate for each subject table. If a single integer is provided, the same number of rows will be generated for each subject table. If a dictionary is provided, the number of rows to generate for each subject table can be specified individually. Default is 4. Ignored if existing_data is provided. Ignored for non-root tables. If a table has a foreign key, the sample size is determined by the corresponding foreign key prompt. If nothing specified, a few rows per parent record are generated.</p> <code>4</code> <code>existing_data</code> <code>dict[str, DataFrame] | None</code> <p>Existing data to augment. If provided, the sample_size argument is ignored. Default is None.</p> <code>None</code> <code>model</code> <code>str</code> <p>The LiteLLM chat completion model to be used. Examples include: - <code>openai/gpt-5-nano</code> (default; fast, and smart) - <code>openai/gpt-5-mini</code> (slower, but smarter) - <code>openai/gpt-5</code> (slowest, but smartest) - <code>gemini/gemini-2.0-flash</code> - <code>gemini/gemini-2.5-flash-preview-04-17</code> - 'groq/gemma2-9b-it<code>-</code>groq/llama-3.3-70b-versatile<code>-</code>anthropic/claude-3-7-sonnet-latest` See https://docs.litellm.ai/docs/providers/ for more options.</p> <code>'openai/gpt-5-nano'</code> <code>api_key</code> <code>str | None</code> <p>The API key to use for the LLM. If not provided, LiteLLM will take it from the environment variables.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to use for the LLM. Default is 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for the LLM. Default is 0.95.</p> <code>0.95</code> <code>n_workers</code> <code>int</code> <p>The number of concurrent workers making the LLM calls. Default is 10. The value is clamped to the range [1, 10]. If n_workers is 1, the generation of batches becomes sequential and certain features for better data consistency are enabled.</p> <code>10</code> <code>return_type</code> <code>Literal['auto', 'dict']</code> <p>The format of the returned data. Default is \"auto\".</p> <code>'auto'</code> <code>progress_callback</code> <code>Callable | None</code> <p>Optional callback function to track progress during data generation. If not provided, a default progress callback will display progress messages in the format: \"Generating table <code>table_name</code>: X%, Y rows, Zs, W.X rows/s\" The callback receives keyword arguments including: table, progress, total, rows, and elapsed_time. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | dict[str, DataFrame]</code> <ul> <li>pd.DataFrame: A single DataFrame containing the generated mock data, if only one table is provided.</li> </ul> <code>DataFrame | dict[str, DataFrame]</code> <ul> <li>dict[str, pd.DataFrame]: A dictionary containing the generated mock data for each table, if multiple tables are provided.</li> </ul> <p>Example of generating mock data for a single table (without PK): <pre><code>from mostlyai import mock\n\ntables = {\n    \"guests\": {\n        \"prompt\": \"Guests of an Alpine ski hotel in Austria\",\n        \"columns\": {\n            \"nationality\": {\"prompt\": \"2-letter code for the nationality\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"first name and last name of the guest\", \"dtype\": \"string\"},\n            \"gender\": {\"dtype\": \"category\", \"values\": [\"male\", \"female\"]},\n            \"age\": {\"prompt\": \"age in years; min: 18, max: 80; avg: 25\", \"dtype\": \"integer\"},\n            \"date_of_birth\": {\"prompt\": \"date of birth\", \"dtype\": \"date\"},\n            \"checkin_time\": {\"prompt\": \"the check in timestamp of the guest; may 2025\", \"dtype\": \"datetime\"},\n            \"is_vip\": {\"prompt\": \"is the guest a VIP\", \"dtype\": \"boolean\"},\n            \"price_per_night\": {\"prompt\": \"price paid per night, in EUR\", \"dtype\": \"float\"},\n            \"room_number\": {\"prompt\": \"room number\", \"dtype\": \"integer\", \"values\": [101, 102, 103, 201, 202, 203, 204]}\n        },\n    }\n}\ndf = mock.sample(tables=tables, sample_size=10, model=\"openai/gpt-5-nano\")\n</code></pre></p> <p>Example of generating mock data for multiple tables (with PK/FK relationships): <pre><code>from mostlyai import mock\n\ntables = {\n    \"customers\": {\n        \"prompt\": \"Customers of a hardware store\",\n        \"columns\": {\n            \"customer_id\": {\"prompt\": \"the unique id of the customer\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"first name and last name of the customer\", \"dtype\": \"string\"},\n        },\n        \"primary_key\": \"customer_id\",  # no composite keys allowed;\n    },\n    \"warehouses\": {\n        \"prompt\": \"Warehouses of a hardware store\",\n        \"columns\": {\n            \"warehouse_id\": {\"prompt\": \"the unique id of the warehouse\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"the name of the warehouse\", \"dtype\": \"string\"},\n        },\n        \"primary_key\": \"warehouse_id\",\n    },\n    \"orders\": {\n        \"prompt\": \"Orders of a Customer\",\n        \"columns\": {\n            \"customer_id\": {\"prompt\": \"the customer id for that order\", \"dtype\": \"string\"},\n            \"warehouse_id\": {\"prompt\": \"the warehouse id for that order\", \"dtype\": \"string\"},\n            \"order_id\": {\"prompt\": \"the unique id of the order\", \"dtype\": \"string\"},\n            \"text\": {\"prompt\": \"order text description\", \"dtype\": \"string\"},\n            \"amount\": {\"prompt\": \"order amount in USD\", \"dtype\": \"float\"},\n        },\n        \"primary_key\": \"order_id\",\n        \"foreign_keys\": [\n            {\n                \"column\": \"customer_id\",\n                \"referenced_table\": \"customers\",\n                \"prompt\": \"each customer has anywhere between 2 and 3 orders\",\n            },\n            {\n                \"column\": \"warehouse_id\",\n                \"referenced_table\": \"warehouses\",\n            },\n        ],\n    },\n    \"items\": {\n        \"prompt\": \"Items in an Order\",\n        \"columns\": {\n            \"item_id\": {\"prompt\": \"the unique id of the item\", \"dtype\": \"string\"},\n            \"order_id\": {\"prompt\": \"the order id for that item\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"the name of the item\", \"dtype\": \"string\"},\n            \"price\": {\"prompt\": \"the price of the item in USD\", \"dtype\": \"float\"},\n        },\n        \"foreign_keys\": [\n            {\n                \"column\": \"order_id\",\n                \"referenced_table\": \"orders\",\n                \"prompt\": \"each order has between 1 and 2 items\",\n            }\n        ],\n    },\n}\ndata = mock.sample(tables=tables, sample_size=2, model=\"openai/gpt-5\")\ndf_customers = data[\"customers\"]\ndf_warehouses = data[\"warehouses\"]\ndf_orders = data[\"orders\"]\ndf_items = data[\"items\"]\n</code></pre></p> <p>Example of auto-increment integer primary keys (self-referencing table): <pre><code>from mostlyai import mock\n\ntables = {\n    \"employees\": {\n        \"prompt\": \"Employees of a company\",\n        \"columns\": {\n            \"employee_id\": {\"dtype\": \"integer\"},  # integer PK \u2192 auto-increment (1, 2, 3, ...)\n            \"name\": {\"prompt\": \"first name and last name of the employee\", \"dtype\": \"string\"},\n            \"boss_id\": {\"dtype\": \"integer\"},  # integer FK \u2192 references auto-incremented values\n            \"role\": {\"prompt\": \"the role of the employee\", \"dtype\": \"string\"},\n        },\n        \"primary_key\": \"employee_id\",\n        \"foreign_keys\": [\n            {\n                \"column\": \"boss_id\",\n                \"referenced_table\": \"employees\",\n                \"prompt\": \"each boss has at most 3 employees\",\n            },\n        ],\n    }\n}\ndf = mock.sample(tables=tables, sample_size=10, model=\"openai/gpt-5\", n_workers=1)\n</code></pre></p> <p>Example of enriching a single dataframe: <pre><code>from mostlyai import mock\nimport pandas as pd\n\ntables = {\n    \"patients\": {\n        \"prompt\": \"Patients of a hospital in Finland\",\n        \"columns\": {\n            \"full_name\": {\"prompt\": \"first name and last name of the patient\", \"dtype\": \"string\"},\n            \"date_of_birth\": {\"prompt\": \"date of birth\", \"dtype\": \"date\"},\n            \"place_of_birth\": {\"prompt\": \"place of birth\", \"dtype\": \"string\"},\n        },\n    },\n}\nexisting_df = pd.DataFrame({\n    \"age\": [25, 30, 35, 40],\n    \"gender\": [\"male\", \"male\", \"female\", \"female\"],\n})\nenriched_df = mock.sample(\n    tables=tables,\n    existing_data={\"patients\": existing_df},\n    model=\"openai/gpt-5-nano\"\n)\nenriched_df\n</code></pre></p> <p>Example of enriching / augmenting an existing dataset: <pre><code>from mostlyai import mock\nimport pandas as pd\n\ntables = {\n    \"customers\": {\n        \"prompt\": \"Customers of a hardware store\",\n        \"columns\": {\n            \"customer_id\": {\"prompt\": \"the unique id of the customer\", \"dtype\": \"string\"},\n            \"name\": {\"prompt\": \"first name and last name of the customer\", \"dtype\": \"string\"},\n            \"email\": {\"prompt\": \"email address of the customer\", \"dtype\": \"string\"},\n            \"phone\": {\"prompt\": \"phone number of the customer\", \"dtype\": \"string\"},\n            \"loyalty_level\": {\"dtype\": \"category\", \"values\": [\"bronze\", \"silver\", \"gold\", \"platinum\"]},\n        },\n        \"primary_key\": \"customer_id\",\n    },\n    \"orders\": {\n        \"prompt\": \"Orders of a Customer\",\n        \"columns\": {\n            \"order_id\": {\"prompt\": \"the unique id of the order\", \"dtype\": \"string\"},\n            \"customer_id\": {\"prompt\": \"the customer id for that order\", \"dtype\": \"string\"},\n            \"order_date\": {\"prompt\": \"the date when the order was placed\", \"dtype\": \"date\"},\n            \"total_amount\": {\"prompt\": \"order amount in USD\", \"dtype\": \"float\"},\n            \"status\": {\"dtype\": \"category\", \"values\": [\"pending\", \"shipped\", \"delivered\", \"cancelled\"]},\n        },\n        \"primary_key\": \"order_id\",\n        \"foreign_keys\": [\n            {\n                \"column\": \"customer_id\",\n                \"referenced_table\": \"customers\",\n                \"prompt\": \"each customer has anywhere between 1 and 3 orders\",\n            },\n        ],\n    },\n}\nexisting_customers = pd.DataFrame({\n    \"customer_id\": [101, 102, 103],\n    \"name\": [\"John Davis\", \"Maria Garcia\", \"Wei Chen\"],\n})\nexisting_orders = pd.DataFrame({\n    \"order_id\": [\"ORD-001\", \"ORD-002\"],\n    \"customer_id\": [101, 101],\n})\ndata = mock.sample(\n    tables=tables,\n    existing_data={\n        \"customers\": existing_customers,\n        \"orders\": existing_orders,\n    },\n    model=\"openai/gpt-5-nano\"\n)\ndf_customers = data[\"customers\"]\ndf_orders = data[\"orders\"]\n</code></pre></p> <p>Example of using a custom progress callback to provide progress in JSON format: <pre><code>from mostlyai import mock\nimport asyncio\nimport json\n\nasync def custom_progress_callback(**kwargs):\n    msg = f\"\n{json.dumps(kwargs)}\"\n    if kwargs[\"progress\"] &lt; kwargs[\"total\"]:\n        print(msg, end=\"\", flush=True)\n    else:\n        print(msg)\n\ndf = mock.sample(\n    tables=tables,\n    sample_size=10,\n    progress_callback=custom_progress_callback\n)\n</code></pre></p>"}]}